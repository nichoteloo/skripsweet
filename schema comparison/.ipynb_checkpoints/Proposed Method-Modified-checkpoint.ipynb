{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from kneed import KneeLocator\n",
    "from numpy import sqrt, square\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Training Data\n",
    "path =r\"E:\\PROJECT\\VSCode Python Project\\Sensor Network\\Uji Coba\\Progress Skripsi\\Data\\Data Nico\\Data 15 April Tanpa Gangguan\\Train 2\"\n",
    "globbed_files = glob.glob(path + \"/*.csv\")\n",
    "data = []\n",
    "for csv in globbed_files:\n",
    "    frame = pd.read_csv(csv)\n",
    "    frame['x'] = os.path.basename(csv).split('.')[0][0]\n",
    "    frame['y'] = os.path.basename(csv).split('.')[0][1]\n",
    "    data.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Load Training Data 2\n",
    "# path2 =r\"E:\\PROJECT\\VSCode Python Project\\Sensor Network\\Uji Coba\\Progress Skripsi\\Data\\Data Nico\\Data 15 April Tanpa Gangguan\\Train 1\"\n",
    "# globbed_files2 = glob.glob(path2 + \"/*.csv\")\n",
    "# for csv in globbed_files2:\n",
    "#     frame = pd.read_csv(csv)\n",
    "#     frame['x'] = os.path.basename(csv).split('.')[0][0]\n",
    "#     frame['y'] = os.path.basename(csv).split('.')[0][1]\n",
    "#     data.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take the lowest number of set data (32)\n",
    "attempt = []\n",
    "for i, item in enumerate(data):\n",
    "    attempt.append(data[i][['Router 1','Router 2','Router 3','Router 4','x','y']]) #.head(32)\n",
    "attempt_concat = pd.concat(attempt)\n",
    "data_train = attempt_concat.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_train[['Router 1','Router 2','Router 3','Router 4']] = data_train[['Router 1','Router 2','Router 3','Router 4']].abs()\n",
    "data_train = data_train.assign(Unique_ID = (data_train['x'].astype(str) + '_' + data_train['y'].astype(str)).astype('category').cat.codes)\n",
    "x_train = data_train.iloc[:,0:4].values\n",
    "y_train = data_train.iloc[:,4:]\n",
    "ref_table = y_train.iloc[:, [0,1,2]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = MinMaxScaler()\n",
    "# scaler.fit(x_train)\n",
    "# x_train = scaler.transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_left =r\"E:\\PROJECT\\VSCode Python Project\\Sensor Network\\Uji Coba\\Progress Skripsi\\Data\\Data Nico\\Data 15 April Tanpa Gangguan\\Test\\Left Test\"\n",
    "globbed_files_left = glob.glob(path_left + \"/*.csv\")\n",
    "data_left = []\n",
    "for csv in globbed_files_left:\n",
    "    frame = pd.read_csv(csv)\n",
    "    frame['x'] = os.path.basename(csv).split('.')[0][0]\n",
    "    frame['y'] = os.path.basename(csv).split('.')[0][1]\n",
    "    data_left.append(frame)\n",
    "    \n",
    "attempt_left = []\n",
    "for i, item in enumerate(data_left):\n",
    "    attempt_left.append(data_left[i][['Router 1','Router 2','Router 3','Router 4','x','y']])\n",
    "attempt_concat_left = pd.concat(attempt_left)\n",
    "data_test_left = attempt_concat_left.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_test_left[['Router 1','Router 2','Router 3','Router 4']] = data_test_left[['Router 1','Router 2','Router 3','Router 4']] .abs()\n",
    "data_test_left = data_test_left.tail(562).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split features and label\n",
    "x_test_left = data_test_left.iloc[:,0:4].values\n",
    "y_test_left = data_test_left.iloc[:,4:]\n",
    "\n",
    "### Normalization\n",
    "# x_test_left = scaler.transform(x_test_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform DB-Kmeans Separate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors = nearest_neighbors.fit(x_train)\n",
    "distances, indices = neighbors.kneighbors(x_train)\n",
    "distances = np.sort(distances[:,4], axis=0)\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=distances[knee.knee], min_samples=5).fit(x_train)\n",
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 24\n",
      "Estimated number of noise points: 57\n"
     ]
    }
   ],
   "source": [
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) # jumlah cluster, total klas - 1 (kalo ada noise)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = pd.DataFrame(x_train,columns=['PC_1','PC_2','PC_3','PC_4'])\n",
    "ref_table = y_train.iloc[:, [0,1,2]].drop_duplicates()\n",
    "y_train['dbscan'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat((x_train_df,y_train),axis=1)\n",
    "\n",
    "combined_core = combined.loc[~(combined['dbscan'] == -1)]\n",
    "combined_core = combined_core.reset_index(drop=True)\n",
    "\n",
    "x_train_cr = combined_core.iloc[:,0:4]\n",
    "y_train_cr = combined_core.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Core and Noises Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_cr = MinMaxScaler()\n",
    "scaler_cr.fit(x_train_cr)\n",
    "x_train_cr = scaler_cr.transform(x_train_cr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_noises = combined.loc[(combined['dbscan'] == -1)]\n",
    "combined_noises = combined_noises.reset_index(drop=True)\n",
    "\n",
    "x_train_n = combined_noises.iloc[:,0:4]\n",
    "y_train_n = combined_noises.iloc[:,4:]\n",
    "\n",
    "scaler_n = MinMaxScaler()\n",
    "scaler_n.fit(x_train_n)\n",
    "x_train_n = scaler_n.transform(x_train_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from kneed import KneeLocator\n",
    "# from sklearn.cluster import KMeans\n",
    "\n",
    "# ks = range(1,25) ## batas maksimal kelas\n",
    "# inertias_values = []  ## or so called SSE\n",
    "# for k in ks:\n",
    "#     model = KMeans(n_clusters=k)     # Create a KMeans instance with k clusters: model\n",
    "#     model.fit(x_train_cr)      # Fit model to samples\n",
    "#     inertias_values.append(model.inertia_)        # Append the inertia to the list of inertias\n",
    "# kl = KneeLocator(range(1,25), inertias_values, curve=\"convex\", direction=\"decreasing\")\n",
    "# kl.elbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = pd.DataFrame(x_train_cr, columns=['PC_1','PC_2','PC_3','PC_4'])\n",
    "# combined_core = pd.concat((temp, y_train_cr), axis=1)\n",
    "\n",
    "# dict_pre = {}\n",
    "# clusters_pre = {}\n",
    "# for x in np.unique(combined_core['dbscan'].values):\n",
    "#     dict_pre[x] = list(combined_core[combined_core.dbscan == x]['Unique_ID'].values)\n",
    "#     clusters_pre[x] = list(combined_core[combined_core.dbscan == x][['PC_1','PC_2','PC_3','PC_4']].values)\n",
    "    \n",
    "# centroid_dbscan = []\n",
    "# for x in clusters_pre:\n",
    "#     centroid_dbscan.append(np.mean(clusters_pre[x], axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Tuple\n",
    "\n",
    "# def _initialize_clusters(k: int) -> dict:\n",
    "#     return {str(i): [] for i in range(1, k + 1)}\n",
    "\n",
    "# def l2norm(point, center) -> float:\n",
    "#     \"\"\"calculate euclidean distance between two n-dimensional points\"\"\"\n",
    "#     s = 0\n",
    "#     for x, y in zip(point, center):\n",
    "#         s += (x - y) ** 2\n",
    "#     d = s[0] ** (1 / 2)\n",
    "#     return d\n",
    "\n",
    "# def l2norm_vector(point, centers) -> np.ndarray:\n",
    "#     \"\"\"calculate l2-norm between a point and several cluster centers.\"\"\"\n",
    "#     l2 = []\n",
    "#     for center in centers:\n",
    "#         center = center.reshape(1, len(center)).T  # (n, 1)\n",
    "#         norm = l2norm(point, center)\n",
    "#         l2.append(norm)\n",
    "#     return np.array(l2)\n",
    "\n",
    "# class Kmeans:\n",
    "#     def __init__(self,\n",
    "#                  n_clusters: int,          # number of clusters assignments\n",
    "#                  data: np.ndarray,         # data matrix (n x d)\n",
    "#                  iterations: int = 100,    # number of iterations to run.\n",
    "#                  centroids: np.ndarray = None\n",
    "#                  ): \n",
    "#         self.n_clusters = n_clusters\n",
    "#         self.data = data\n",
    "#         self.iterations = iterations\n",
    "#         self.clusters = _initialize_clusters(self.n_clusters) \n",
    "#         self.centroids = np.array(centroids)\n",
    "# #         index = np.random.choice(self.data.shape[0], self.n_clusters, replace=False)\n",
    "# #         self.centroids = self.data[:,:4][index]\n",
    "        \n",
    "#     def train(self, method: str = \"l2\", verbose: bool = True) -> Tuple[np.ndarray, dict]:\n",
    "#         iterations = self.iterations  # allows for multiple training runs if user desires\n",
    "        \n",
    "#         if method == \"l2\":\n",
    "#             self._train_l2(iterations)\n",
    "#         elif method == \"l1\":\n",
    "#             self._train_l1(iterations, verbose=verbose)\n",
    "            \n",
    "#         if verbose:\n",
    "#             print(\"Training done...\")\n",
    "#         return self.centroids, self.clusters\n",
    "    \n",
    "#     def predict(self, x: np.ndarray, method: str = \"l2\") -> Tuple[dict, np.ndarray]:\n",
    "#         \"\"\"calculate closest distance between input matrix and cluster assignments\"\"\"\n",
    "#         # initialize cluster dictionary.\n",
    "#         #clusters = _initialize_clusters(self.n_clusters)\n",
    "#         if method == \"l2\":\n",
    "#             for i in range(len(x)):\n",
    "#                 d = l2norm_vector(x[i], self.centroids)\n",
    "#                 c = np.argmin(d) + 1\n",
    "#                 #clusters[str(c)].append(x[i])\n",
    "#         elif method == \"l1\":\n",
    "#             for i in range(len(x)):\n",
    "#                 d = l1norm_vector(x[i], self.centroids)\n",
    "#                 c = np.argmin(d) + 1\n",
    "#                 #clusters[str(c)].append(x[i])            \n",
    "#         return c #clusters, self.centroids\n",
    "                \n",
    "#     def _train_l2(self, iterations: int):\n",
    "#         \"\"\"run training using l2-norm\"\"\"\n",
    "#         i = 0\n",
    "#         while iterations != 0:\n",
    "#             for j in range(self.n_clusters):\n",
    "#                 self.clusters[str(j + 1)] = []\n",
    "#             for i in range(len(self.data)):\n",
    "#                 d = l2norm_vector(self.data[:,:4][i], self.centroids)\n",
    "#                 c = np.argmin(d) + 1\n",
    "#                 self.clusters[str(c)].append((self.data[:,:4][i], self.data[:,4][i]))\n",
    "\n",
    "#             old_centroids = self.centroids.copy()\n",
    "#             for i, c in enumerate(self.clusters.keys()):\n",
    "#                 mean = np.mean(np.array([x[0] for x in self.clusters[str(c)]]), axis=0)\n",
    "#                 self.centroids[int(i)] = mean\n",
    "\n",
    "#             print(\"\\riterations: {}...\".format(iterations))\n",
    "#             iterations -= 1\n",
    "#             i += 1\n",
    "#             if np.all(old_centroids == self.centroids):\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = np.append(x_train_cr, \n",
    "#                  y_train_cr['Unique_ID'].values.astype(int).reshape(y_train_cr['Unique_ID'].shape[0],1), \n",
    "#                  axis=1)\n",
    "# K_l2 = Kmeans(n_clusters = 24, data = data, centroids = centroid_dbscan)\n",
    "# centroids_l2, clusters_l2 = K_l2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def core_combine(clusters, ref_table):\n",
    "#     value_ = []\n",
    "#     unique_ = []\n",
    "#     keys_ = []\n",
    "#     for key,values in clusters.items():\n",
    "#         for value in values:\n",
    "#             value_.append(value[0]) \n",
    "#             unique_.append(value[1])\n",
    "#             keys_.append(key)\n",
    "    \n",
    "#     df_unique_keys = pd.DataFrame({'Unique_ID': unique_,'kmeans': keys_}).astype(int)\n",
    "\n",
    "#     coord_ = []\n",
    "#     for x in df_unique_keys['Unique_ID'].values:\n",
    "#         coord_.append(ref_table[ref_table.Unique_ID == x][['x','y']].values.reshape(2,))\n",
    "\n",
    "#     df_coord = pd.DataFrame(np.array(coord_),columns=['x','y'])\n",
    "#     df_rssi_value = pd.DataFrame(value_,columns=['PC_1','PC_2','PC_3','PC_4'])\n",
    "#     combined_cr = pd.concat([df_rssi_value, df_coord, df_unique_keys],axis=1)\n",
    "    \n",
    "#     return combined_cr\n",
    "\n",
    "# combined_cr = core_combine(clusters_l2, ref_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "final_model = KMeans(n_clusters = 4, random_state = 1)\n",
    "final_model.fit(x_train_cr)\n",
    "\n",
    "y_train_cr['kmeans'] = final_model.labels_\n",
    "x_train_df = pd.DataFrame(x_train_cr,columns=['PC_1','PC_2','PC_3','PC_4'])\n",
    "combined_cr = pd.concat((x_train_df,y_train_cr),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Clusters (David Bouldin Internal Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def arrayGen(centroids, clusters):\n",
    "#     dataframe = []\n",
    "#     labels = []\n",
    "#     for i, (key,values) in enumerate(clusters.items()):\n",
    "#         for x, point in enumerate(values):\n",
    "#             dataframe.append(clusters[key][x])\n",
    "#             labels.append(key)\n",
    "#     dataframe_array = np.array(dataframe)\n",
    "#     labels_array = np.array(labels)\n",
    "#     return dataframe_array, labels_array\n",
    "\n",
    "# def compute_s(i, x, labels, clusters):\n",
    "#     norm_c= len(clusters)\n",
    "#     s = 0\n",
    "#     for x in clusters:\n",
    "#         s += distance.euclidean(x, clusters[i])\n",
    "#     return s\n",
    "\n",
    "# def compute_Rij(i, j, x, labels, clusters, nc):\n",
    "#     Rij = 0\n",
    "#     try:\n",
    "#         d = distance.euclidean(clusters[i],clusters[j])\n",
    "#         Rij = (compute_s(i, x, labels, clusters) + compute_s(j, x, labels, clusters))/d\n",
    "#     except:\n",
    "#         Rij = 0\n",
    "#     return Rij\n",
    "\n",
    "# def compute_R(i, x, labels, clusters, nc): \n",
    "#     list_r = []\n",
    "#     for i in range(nc):\n",
    "#         for j in range(nc):\n",
    "#             if(i!=j):\n",
    "#                 temp = compute_Rij(i, j, x, labels, clusters, nc)\n",
    "#                 list_r.append(temp)\n",
    "#     return max(list_r)\n",
    "\n",
    "# def compute_DB_index(x, labels, clusters, nc):\n",
    "#     sigma_R = 0.0\n",
    "#     for i in range(nc):\n",
    "#         sigma_R = sigma_R + compute_R(i, x, labels, clusters, nc)\n",
    "#     DB_index = float(sigma_R)/float(nc)\n",
    "#     return DB_index\n",
    "\n",
    "# ## features, labels, centroids, amount of clusters\n",
    "# n = kl.elbow\n",
    "# dataframe_array, labels_array = arrayGen(centroid, clusters)\n",
    "# index_db_val = compute_DB_index(dataframe_array, labels_array, centroid, n)\n",
    "# print (\"The value of Davies Bouldin index for a K-Means cluser of size \" + str(n) + \" is: \" + str(index_db_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Dict_ and Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {}\n",
    "clusters = {}\n",
    "for x in np.unique(combined_cr['kmeans'].values):\n",
    "    dict_[x] = list(combined_cr[combined_cr.kmeans == x]['Unique_ID'].values)\n",
    "    clusters[x] = list(combined_cr[combined_cr.kmeans == x][['PC_1','PC_2','PC_3','PC_4']].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Imbalaced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_dict(y):\n",
    "    '''\n",
    "    balancing for oversampling strategy\n",
    "    '''\n",
    "    new_strategy = {}\n",
    "    keys = Counter(y).keys()\n",
    "    values = max(Counter(y).values())\n",
    "    for key in keys:\n",
    "        new_strategy[key] = values\n",
    "    return new_strategy\n",
    "\n",
    "## untuk bantu train regressor\n",
    "def over_sampling(clusters, dict_, index):\n",
    "    '''\n",
    "    random oversampling strategy\n",
    "    '''\n",
    "    x_total = {}\n",
    "    y_total = {}\n",
    "    for i,item in enumerate(index):\n",
    "        if len(item) > 1:\n",
    "            x = [z for z in clusters[list(clusters.keys())[i]]]\n",
    "            y = [z for z in dict_[list(dict_.keys())[i]]]\n",
    "            strategy = strategy_dict(y)\n",
    "            oversample = RandomOverSampler(sampling_strategy=strategy)\n",
    "            x_over, y_over = oversample.fit_resample(x, y)\n",
    "            x_total[list(clusters.keys())[i]] = np.array(x_over)\n",
    "            y_total[list(clusters.keys())[i]] = np.array(y_over)\n",
    "        else:\n",
    "            x_total[list(clusters.keys())[i]] = np.array([z for z in clusters[list(clusters.keys())[i]]])\n",
    "            y_total[list(clusters.keys())[i]] = np.array([z for z in dict_[list(dict_.keys())[i]]])\n",
    "    return x_total, y_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = [list(np.unique(x)) for x in dict_.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_total, y_total = over_sampling(clusters, dict_, unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catch Sensitive and Insensitive Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_insensitive = {}\n",
    "y_insensitive = {}\n",
    "for x in [i for i, x in enumerate(unique) if len(x) > 1]:\n",
    "    x_insensitive[x] = x_total[x]\n",
    "    y_insensitive[x] = y_total[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## matching process between new id and ref table\n",
    "data_balanced = {}\n",
    "for i, (x,y) in enumerate(zip(x_insensitive, y_insensitive)):\n",
    "    data_balanced[\"{0}\".format(list(x_insensitive.keys())[i])] = y_insensitive[y]\n",
    "data_df_balanced = pd.DataFrame.from_dict(data_balanced, orient='index').T\n",
    "\n",
    "# ## matching process between new id and ref table\n",
    "# data_balanced = {}\n",
    "# for i, (x,y) in enumerate(zip(x_total, y_total)):\n",
    "#     data_balanced[\"{0}\".format(list(x_total.keys())[i])] = y_total[y]\n",
    "# data_df_balanced = pd.DataFrame.from_dict(data_balanced, orient='index').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_list_id(df_id, ref_table):\n",
    "    dict_loc = {}; id_total = []\n",
    "    m_total = ref_table.shape[0]\n",
    "    poses = []\n",
    "    for c in df_id:\n",
    "        x = []\n",
    "        for i in range(len(df_id[c].dropna())):\n",
    "            x.append(int(df_id[c][i]))\n",
    "        var = np.array(x)\n",
    "        id_total.append(var)\n",
    "    for i in range(m_total):\n",
    "        key = int(ref_table.iloc[i]['Unique_ID'])\n",
    "        value = ref_table.iloc[i, 0:2].values\n",
    "        dict_loc[key] = value\n",
    "    for i in range(len(id_total)):\n",
    "        pos = []\n",
    "        for j in range(len(id_total[i])):\n",
    "            x = id_total[i][j]\n",
    "            pos.append(dict_loc.get(x))\n",
    "        pos = np.array(pos)\n",
    "        poses.append(pos)    \n",
    "    return id_total, poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_total_balanced, poses_balanced = filter_list_id(data_df_balanced, ref_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(id_total,poses,i):\n",
    "    df_id = pd.DataFrame(id_total[i],columns=[str(i)])\n",
    "    df_pos = pd.DataFrame(poses[i],columns=['x','y'])\n",
    "    return df_pos, df_id\n",
    "\n",
    "df = {}\n",
    "for i, (_id,pose) in enumerate(zip(id_total_balanced,poses_balanced)):\n",
    "    df[list(x_insensitive.keys())[i]] = pd.concat((make_df(id_total_balanced,poses_balanced,i)),axis=1)\n",
    "\n",
    "# df = {}\n",
    "# for i, (_id,pose) in enumerate(zip(id_total_balanced,poses_balanced)):\n",
    "#     df[list(x_total.keys())[i]] = pd.concat((make_df(id_total_balanced,poses_balanced,i)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained and Tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regression model for pose prediction\n",
    "def tuning_regr_knn(x_train,y_train):\n",
    "    QUANTITATIVE_COLUMNS = ['x', 'y']\n",
    "    regr = KNeighborsRegressor(n_neighbors=1)\n",
    "\n",
    "    metric = ['euclidean']\n",
    "    param_grid = {'metric': metric}\n",
    "    \n",
    "    grid = GridSearchCV(estimator = regr,\n",
    "                        param_grid = param_grid,\n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        cv = 10,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "    tic = time.time()\n",
    "    grid_result_regr = grid.fit(x_train,y_train[QUANTITATIVE_COLUMNS].values.astype(np.float64))\n",
    "    toc = time.time()\n",
    "    run_time = (toc - tic)/60\n",
    "    return grid_result_regr.best_estimator_, grid_result_regr.best_score_, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regr_tuned = {}\n",
    "# for i,(x,y) in enumerate(zip(x_total,df)):\n",
    "#     regr, regr_score, runtime_regr = tuning_regr_knn(x_total[x], df[y])\n",
    "#     regr_tuned[\"regr_{0}\".format(list(x_total.keys())[i])] = regr, regr_score, runtime_regr\n",
    "\n",
    "regr_tuned = {}\n",
    "for i,(x,y) in enumerate(zip(x_insensitive,df)):\n",
    "    regr, regr_score, runtime_regr = tuning_regr_knn(x_insensitive[x], df[y])\n",
    "    regr_tuned[\"regr_{0}\".format(list(x_insensitive.keys())[i])] = regr, regr_score, runtime_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_left['Unique_ID'] = [int(ref_table[(ref_table.x == str(x[0])) & (ref_table.y == str(x[1]))]['Unique_ID'].values) \n",
    "                              for x in y_test_left.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def nn_modified(test_noises, x_train, y_train):\n",
    "    distance_n = []\n",
    "    for x,y in zip(x_train, y_train):\n",
    "        dim = np.array(x).shape[0]\n",
    "        eu_distance = sp.spatial.distance.euclidean(test_noises.reshape(1,dim),np.array(x).reshape(1,dim))\n",
    "        distance_n.append((eu_distance,y))\n",
    "    distance_n = sorted(distance_n)[0][1]\n",
    "    return distance_n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use WKNN instead, return label\n",
    "clus_dbscan = []\n",
    "for n in x_test_left:\n",
    "    clus_dbscan.append(nn_modified(n, x_train, y_train['dbscan'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate index_core and noises from function with cosine similiarity\n",
    "def index_nc(labels_):\n",
    "    index_core = []\n",
    "    index_noises = []\n",
    "    for i, x in enumerate(labels_):\n",
    "        if x == -1:\n",
    "            index_noises.append(i) \n",
    "        else:\n",
    "            index_core.append(i)\n",
    "    return index_core, index_noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_core, index_noises = index_nc(clus_dbscan)\n",
    "\n",
    "test_core = x_test_left[index_core] ## core point testing\n",
    "test_noises = x_test_left[index_noises]\n",
    "\n",
    "label_core = y_test_left.values[index_core] ## core point testing\n",
    "label_noises = y_test_left.values[index_noises]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Core and Noises Test Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_core = scaler_cr.transform(test_core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_noises = scaler_cr.transform(test_noises)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Point Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wknn_modified(test_noises, x_train_n, y_train_n, k):\n",
    "    distance_n = []; freq = defaultdict(float)\n",
    "    for x,y in zip(x_train_n, y_train_n):\n",
    "        dim = np.array(x).shape[0]\n",
    "        euclidean_distance = sp.spatial.distance.cosine(test_noises.reshape(1,dim),np.array(x).reshape(1,dim))\n",
    "        distance_n.append((euclidean_distance,y))\n",
    "    distance_n = sorted(distance_n)\n",
    "    unique_c = list(np.unique(np.array(distance_n)[:,1]).astype(int))\n",
    "    for x in unique_c:\n",
    "        freq.setdefault(int(x),0)\n",
    "    for d in distance_n:\n",
    "        temp = freq[float(d[1])]\n",
    "        if d[0] == float(0):\n",
    "            temp_ = float(temp)\n",
    "            freq[float(d[1])] = temp_\n",
    "        else:\n",
    "            temp_ = float(temp) + (1 / d[0])\n",
    "            freq[float(d[1])] = temp_\n",
    "    for key in freq:\n",
    "        freq[key] = freq[key]/k\n",
    "    return max(freq, key=lambda key: freq[key]), distance_n\n",
    "\n",
    "def test_prediction(x_test_left, clus_, dict_, x_insensitive, regr_tuned, ref_table, x_train_cr, y_train_cr):\n",
    "    result = []\n",
    "    for i, (x,y) in enumerate(zip(x_test_left, clus_)):\n",
    "        if y in list(x_insensitive.keys()):\n",
    "            data = x.reshape(1,len(x))\n",
    "            result.append(regr_tuned['regr_{0}'.format(int(y))][0].predict(data).astype(int).reshape(2,))\n",
    "        else:\n",
    "            y_ = np.unique(dict_[y])\n",
    "            result.append(ref_table[ref_table.Unique_ID == int(y_)][['x','y']].values.astype(int).reshape(2,))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_kmeans = []\n",
    "for x in test_core:\n",
    "    clus_kmeans.append(final_model.predict(x.reshape(1,4)))\n",
    "\n",
    "# clus_kmeans = []\n",
    "# for x in test_core:\n",
    "#     clus_kmeans.append(K_l2.predict(x.reshape(1,4)))\n",
    "    \n",
    "clus_core = [int(i) for i in clus_kmeans]\n",
    "\n",
    "result = test_prediction(test_core, clus_core, dict_, x_insensitive, regr_tuned, \n",
    "                         ref_table, x_train_cr, y_train_cr)\n",
    "\n",
    "df_pred = pd.DataFrame(result, columns=['x_pred','y_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Point Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### With Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Physical Distance Calculation\n",
    "benchmark = {\n",
    "    1 : [(str(0),str(1)),(str(1),str(0))],\n",
    "    2 : [(str(3),str(0)),(str(4),str(1))],\n",
    "    3 : [(str(4),str(3)),(str(3),str(4))],\n",
    "    4 : [(str(1),str(4)),(str(0),str(3))], \n",
    "}\n",
    "\n",
    "def physical_distance(x, data_train, ref_table, benchmark, ap_sel):\n",
    "    d = []\n",
    "    clos = []\n",
    "    \n",
    "    bm_1 = round(np.mean(data_train[(data_train.x == benchmark[ap_sel][0][0]) & \n",
    "                                    (data_train.y == benchmark[ap_sel][0][1])].values[:,ap_sel-1]))\n",
    "    bm_2 = round(np.mean(data_train[(data_train.x == benchmark[ap_sel][1][0]) & \n",
    "                                    (data_train.y == benchmark[ap_sel][1][1])].values[:,ap_sel-1]))\n",
    "    \n",
    "    d_1 = 1 * (10 ** (( bm_1 - x ) / ( 10 * 3.1 )))\n",
    "    d_2 = 1 * (10 ** (( bm_2 - x ) / ( 10 * 3.1 )))\n",
    "    \n",
    "    for j in ref_table.values[:,-1]:\n",
    "        temp = round(np.mean(data_train[data_train.Unique_ID == j].values[:, ap_sel-1].astype(int)))\n",
    "        ref_dis_1 = (1 * (10 ** (( bm_1 - temp ) / ( 10 * 3.1 ))), j)\n",
    "        ref_dis_2 = (1 * (10 ** (( bm_2 - temp ) / ( 10 * 3.1 ))), j)\n",
    "        d.append((ref_dis_1, ref_dis_2))\n",
    "    \n",
    "    for z in d:\n",
    "        clos.append((sp.spatial.distance.cityblock(d_1, z[0][0]), z[0][1]))\n",
    "        clos.append((sp.spatial.distance.cityblock(d_2, z[1][0]), z[1][1]))\n",
    "        \n",
    "    return sorted(clos)[:25]\n",
    "\n",
    "\n",
    "phy_test = data_test_left.values[:,:4][index_noises]\n",
    "data_train_new = pd.DataFrame(data_train.values[combined.loc[~(combined['dbscan'] == -1)]\n",
    "                                                .index.values.tolist()], columns = data_train.columns)\n",
    "\n",
    "phy_dis = []\n",
    "for features in phy_test:\n",
    "    temp = []\n",
    "    for i, feature in enumerate(features):\n",
    "        ap_sel = i + 1\n",
    "        temp.append(physical_distance(feature, data_train_new, ref_table, benchmark, ap_sel))\n",
    "    phy_dis.append(temp)\n",
    "    \n",
    "unique_phy = [[e[0] for e in Counter([c[1] for co in con for c in co]).most_common(4)] for con in phy_dis]\n",
    "\n",
    "distance_phy_sel = [[d for dis in item for d in dis if d[1] in u] for item, u in zip(phy_dis, unique_phy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Spatial Distance Calculation\n",
    "x_train_new = x_train_cr\n",
    "y_train_new = y_train_cr['Unique_ID'].values \n",
    "\n",
    "def spatial(test_noises, x_train_n, y_train_n, k=25):\n",
    "    distance_n = []\n",
    "    for x,y in zip(x_train_n, y_train_n):\n",
    "        dim = np.array(x).shape[0]\n",
    "        euclidean_distance = sp.spatial.distance.cosine(test_noises.reshape(1,dim),np.array(x).reshape(1,dim))\n",
    "        distance_n.append((euclidean_distance,y))\n",
    "    distance_n = sorted(distance_n)[:k]\n",
    "    return distance_n\n",
    "\n",
    "distance_sp_sel = [spatial(n, x_train_new, y_train_new) for n in test_noises]\n",
    "\n",
    "unique_sp = [[d[1] for d in dis] for dis in distance_sp_sel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_unique(unique_phy, unique_sp, distance_phy_sel, distance_sp_sel):\n",
    "    example = []\n",
    "    for i, (p, s, dp, ds) in enumerate(zip(unique_phy, unique_sp, distance_phy_sel, distance_sp_sel)):\n",
    "        if (len(set(p) & set(s)) == 1):\n",
    "            example.append(list(set(p) & set(s))[0])\n",
    "        elif (len(set(p) & set(s)) > 1):\n",
    "            buff = sorted([d for d in ds if d[1] in list(set(p) & set(s))])\n",
    "            example.append(buff[0][1])\n",
    "        else:\n",
    "            example.append(sorted([d for d in ds])[0][1])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "unique_tot = get_unique(unique_phy, unique_sp, distance_phy_sel, distance_sp_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_n = []\n",
    "for x in unique_tot:\n",
    "    pred_n.append((int(ref_table['x'][ref_table[ref_table.Unique_ID==x].index]),\n",
    "                   int(ref_table['y'][ref_table[ref_table.Unique_ID==x].index])))\n",
    "    \n",
    "dataset_n = pd.DataFrame(pred_n,columns=['x_pred','y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_tot = []\n",
    "# for p, s in zip(unique_phy, unique_sp):\n",
    "#     unique_tot.append(p+s)\n",
    "\n",
    "# clus_wknn_n = []\n",
    "# dis_wknn_n = []\n",
    "# for n, up in zip(test_noises, unique_phy):\n",
    "#     df = combined_cr[combined_cr['Unique_ID'].isin(up)].reset_index(drop=True)\n",
    "#     x_train_phy = df.values[:,:4].astype(float)\n",
    "#     y_train_phy = df['Unique_ID'].values.astype(int)\n",
    "#     clus, distance = wknn_modified(n, x_train_phy, y_train_phy, x_train_phy.shape[0])\n",
    "#     clus_wknn_n.append(clus)\n",
    "#     dis_wknn_n.append(distance)\n",
    "\n",
    "# def get_weight_fuse(distance1, distance2):\n",
    "#     freq = {}\n",
    "#     buff = distance1 + distance2\n",
    "#     count = Counter([b[1] for b in buff])\n",
    "#     for b in buff:\n",
    "#         freq.setdefault(b[1],0)\n",
    "#     for d in buff:\n",
    "#         temp = freq[float(d[1])]\n",
    "#         if d[0] == float(0):\n",
    "#             temp_ = float(temp)\n",
    "#             freq[float(d[1])] = temp_\n",
    "#         else:\n",
    "#             temp_ = float(temp) + (1 / d[0])\n",
    "#             freq[float(d[1])] = temp_\n",
    "#     for key in freq:\n",
    "#         freq[key] = freq[key] / count[key]\n",
    "#     return max(freq, key=lambda key: freq[key]), freq\n",
    "\n",
    "# phy_d = {}\n",
    "# for i,x in enumerate(data_test_left.values[:,:4][index_noises]):\n",
    "#     phy_d[i] = physical_distance(x, data_train, ref_table, benchmark)\n",
    "    \n",
    "# distance_new = {}\n",
    "# for i,n in enumerate(test_noises):\n",
    "#     distance_new[i] = wknn_modified_2(n, x_train_new, y_train_new)\n",
    "\n",
    "# clus_fuse = []\n",
    "# for x,y in zip(phy_d.items(), distance_new.items()):\n",
    "#     clus_fuse.append(get_weight_fuse(x[1],y[1]))\n",
    "\n",
    "# pred_n = []\n",
    "# for x in clus_fuse:\n",
    "#     pred_n.append((int(ref_table['x'][ref_table[ref_table.Unique_ID==x].index]),\n",
    "#                    int(ref_table['y'][ref_table[ref_table.Unique_ID==x].index])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Without Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_new = x_train_cr\n",
    "# y_train_new = y_train_cr['Unique_ID'].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## use WKNN instead, return label\n",
    "# clus_wknn_n = []\n",
    "# dis_wknn_n = []\n",
    "# for n in test_noises:\n",
    "#     clus, distance = wknn_modified(n, x_train_new, y_train_new, 5)\n",
    "#     clus_wknn_n.append(clus)\n",
    "#     dis_wknn_n.append(distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_n = []\n",
    "# for x in clus_wknn_n:\n",
    "#     pred_n.append((int(ref_table['x'][ref_table[ref_table.Unique_ID==x].index]),\n",
    "#                    int(ref_table['y'][ref_table[ref_table.Unique_ID==x].index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_n = pd.DataFrame(pred_n,columns=['x_pred','y_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Regression Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regr_error(df_final,label_core):\n",
    "    x, y  = df_final['x_pred'].values, df_final['y_pred'].values\n",
    "    x0, y0 = label_core[:,0].astype(int), label_core[:,1].astype(int)\n",
    "    coords_error = np.sqrt(np.square(x - x0) + np.square(y - y0))\n",
    "    mean_loc_error = coords_error.mean()\n",
    "    return mean_loc_error, coords_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4706382031574066\n"
     ]
    }
   ],
   "source": [
    "mean_loc_error, coords_error = calculate_regr_error(df_pred,label_core)\n",
    "print(mean_loc_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.7731625988476911\n"
     ]
    }
   ],
   "source": [
    "# noises point error\n",
    "mean_loc_error_n, coords_error_n = calculate_regr_error(dataset_n,label_noises)\n",
    "print(mean_loc_error_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.3788616875655784"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Error gabungan\n",
    "np.sum((len(label_core) * mean_loc_error)+(len(label_noises) * mean_loc_error_n)) / (len(label_core) + len(label_noises))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NicolasEnv] *",
   "language": "python",
   "name": "conda-env-NicolasEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
