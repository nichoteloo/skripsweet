{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import scipy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from kneed import KneeLocator\n",
    "from numpy import sqrt, square\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Training Data\n",
    "path =r\"E:\\PROJECT\\VSCode Python Project\\Sensor Network\\Uji Coba\\Progress Skripsi\\Data\\Data Nico\\Data 15 April Tanpa Gangguan\\Train 2\"\n",
    "globbed_files = glob.glob(path + \"/*.csv\")\n",
    "data = []\n",
    "for csv in globbed_files:\n",
    "    frame = pd.read_csv(csv)\n",
    "    frame['x'] = os.path.basename(csv).split('.')[0][0]\n",
    "    frame['y'] = os.path.basename(csv).split('.')[0][1]\n",
    "    data.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take the lowest number of set data (32)\n",
    "attempt = []\n",
    "for i, item in enumerate(data):\n",
    "    attempt.append(data[i][['Router 1','Router 2','Router 3','Router 4','x','y']]) #.head(32)\n",
    "attempt_concat = pd.concat(attempt)\n",
    "data_train = attempt_concat.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_train[['Router 1','Router 2','Router 3','Router 4']] = data_train[['Router 1','Router 2','Router 3','Router 4']].abs()\n",
    "data_train = data_train.assign(Unique_ID = (data_train['x'].astype(str) + '_' + data_train['y'].astype(str)).astype('category').cat.codes)\n",
    "x_train = data_train.iloc[:,0:4].values\n",
    "y_train = data_train.iloc[:,4:]\n",
    "ref_table = y_train.iloc[:, [0,1,2]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(x_train)\n",
    "x_train = scaler.transform(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_left =r\"E:\\PROJECT\\VSCode Python Project\\Sensor Network\\Uji Coba\\Progress Skripsi\\Data\\Data Nico\\Data 15 April Tanpa Gangguan\\Test\\Right Test\"\n",
    "globbed_files_left = glob.glob(path_left + \"/*.csv\")\n",
    "data_left = []\n",
    "for csv in globbed_files_left:\n",
    "    frame = pd.read_csv(csv)\n",
    "    frame['x'] = os.path.basename(csv).split('.')[0][0]\n",
    "    frame['y'] = os.path.basename(csv).split('.')[0][1]\n",
    "    data_left.append(frame)\n",
    "    \n",
    "attempt_left = []\n",
    "for i, item in enumerate(data_left):\n",
    "    attempt_left.append(data_left[i][['Router 1','Router 2','Router 3','Router 4','x','y']])\n",
    "attempt_concat_left = pd.concat(attempt_left)\n",
    "data_test_left = attempt_concat_left.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_test_left[['Router 1','Router 2','Router 3','Router 4']] = data_test_left[['Router 1','Router 2','Router 3','Router 4']].abs()\n",
    "data_test_left = data_test_left.tail(562).reset_index(drop=True)\n",
    "\n",
    "## split features and label\n",
    "x_test_left = data_test_left.iloc[:,0:4].values\n",
    "y_test_left = data_test_left.iloc[:,4:]\n",
    "\n",
    "### Normalization\n",
    "x_test_left = scaler.transform(x_test_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform DB-Kmeans Separate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Perform DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors = nearest_neighbors.fit(x_train)\n",
    "distances, indices = neighbors.kneighbors(x_train)\n",
    "distances = np.sort(distances[:,4], axis=0)\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=distances[knee.knee], min_samples=5).fit(x_train)\n",
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 24\n",
      "Estimated number of noise points: 69\n"
     ]
    }
   ],
   "source": [
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) # jumlah cluster, total klas - 1 (kalo ada noise)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = pd.DataFrame(x_train,columns=['PC_1','PC_2','PC_3','PC_4'])\n",
    "ref_table = y_train.iloc[:, [0,1,2]].drop_duplicates()\n",
    "y_train['dbscan'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat((x_train_df,y_train),axis=1)\n",
    "\n",
    "combined_core = combined.loc[~(combined['dbscan'] == -1)]\n",
    "combined_core = combined_core.reset_index(drop=True)\n",
    "\n",
    "combined_noise = combined.loc[combined['dbscan'] == -1]\n",
    "combined_noise = combined_noise.reset_index(drop=True)\n",
    "\n",
    "x_train_cr = combined_core.iloc[:,0:4]\n",
    "y_train_cr = combined_core.iloc[:,4:]\n",
    "\n",
    "x_train_n = combined_noise.iloc[:,0:4]\n",
    "y_train_n = combined_noise.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "ks = range(1, 25)\n",
    "inertias_values = []  ## or so called SSE\n",
    "for k in ks:\n",
    "    model = KMeans(n_clusters=k)     # Create a KMeans instance with k clusters: model\n",
    "    model.fit(x_train_cr)      # Fit model to samples\n",
    "    inertias_values.append(model.inertia_)        # Append the inertia to the list of inertias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kneed import KneeLocator\n",
    "kl = KneeLocator(range(1, 25), inertias_values, curve=\"convex\", direction=\"decreasing\")\n",
    "\n",
    "final_model = KMeans(n_clusters = 4,random_state=1)\n",
    "final_model.fit(x_train_cr)\n",
    "\n",
    "y_train_cr['kmean'] = final_model.labels_\n",
    "centroid = final_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Clusters (David Bouldin Internal Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Clusters finals after kmeans\n",
    "combined_cr = pd.concat([x_train_cr,y_train_cr],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {}\n",
    "clusters = {}\n",
    "for x in np.unique(combined_cr['kmean'].values):\n",
    "    dict_[x] = list(combined_cr[combined_cr.kmean == x]['Unique_ID'].values)\n",
    "    clusters[x] = list(combined_cr[combined_cr.kmean == x][['PC_1','PC_2','PC_3','PC_4']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The value of Davies Bouldin index for a K-Means cluser of size 8 is: 0.0\n"
     ]
    }
   ],
   "source": [
    "def arrayGen(centroids, clusters):\n",
    "    dataframe = []\n",
    "    labels = []\n",
    "    for i, (key,values) in enumerate(clusters.items()):\n",
    "        for x, point in enumerate(values):\n",
    "            dataframe.append(clusters[key][x])\n",
    "            labels.append(key)\n",
    "    dataframe_array = np.array(dataframe)\n",
    "    labels_array = np.array(labels)\n",
    "    return dataframe_array, labels_array\n",
    "\n",
    "def compute_s(i, x, labels, clusters):\n",
    "    norm_c= len(clusters)\n",
    "    s = 0\n",
    "    for x in clusters:\n",
    "        s += distance.euclidean(x, clusters[i])\n",
    "    return s\n",
    "\n",
    "def compute_Rij(i, j, x, labels, clusters, nc):\n",
    "    Rij = 0\n",
    "    try:\n",
    "        d = distance.euclidean(clusters[i],clusters[j])\n",
    "        Rij = (compute_s(i, x, labels, clusters) + compute_s(j, x, labels, clusters))/d\n",
    "    except:\n",
    "        Rij = 0\n",
    "    return Rij\n",
    "\n",
    "def compute_R(i, x, labels, clusters, nc): \n",
    "    list_r = []\n",
    "    for i in range(nc):\n",
    "        for j in range(nc):\n",
    "            if(i!=j):\n",
    "                temp = compute_Rij(i, j, x, labels, clusters, nc)\n",
    "                list_r.append(temp)\n",
    "    return max(list_r)\n",
    "\n",
    "def compute_DB_index(x, labels, clusters, nc):\n",
    "    sigma_R = 0.0\n",
    "    for i in range(nc):\n",
    "        sigma_R = sigma_R + compute_R(i, x, labels, clusters, nc)\n",
    "    DB_index = float(sigma_R)/float(nc)\n",
    "    return DB_index\n",
    "\n",
    "## features, labels, centroids, amount of clusters\n",
    "n = kl.elbow\n",
    "dataframe_array, labels_array = arrayGen(centroid, clusters)\n",
    "index_db_val = compute_DB_index(dataframe_array, labels_array, centroid, n)\n",
    "print (\"The value of Davies Bouldin index for a K-Means cluser of size \" + str(n) + \" is: \" + str(index_db_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Imbalaced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_dict(y):\n",
    "    '''\n",
    "    balancing for oversampling strategy\n",
    "    '''\n",
    "    new_strategy = {}\n",
    "    keys = Counter(y).keys()\n",
    "    values = max(Counter(y).values())\n",
    "    for key in keys:\n",
    "        new_strategy[key] = values\n",
    "    return new_strategy\n",
    "\n",
    "## untuk bantu train regressor\n",
    "def over_sampling(clusters, dict_, index):\n",
    "    '''\n",
    "    random oversampling strategy\n",
    "    '''\n",
    "    x_total = {}\n",
    "    y_total = {}\n",
    "    for i,item in enumerate(index):\n",
    "        if len(item) > 1:\n",
    "            x = [z for z in clusters[list(clusters.keys())[i]]]\n",
    "            y = [z for z in dict_[list(dict_.keys())[i]]]\n",
    "            strategy = strategy_dict(y)\n",
    "            oversample = RandomOverSampler(sampling_strategy=strategy)\n",
    "            x_over, y_over = oversample.fit_resample(x, y)\n",
    "            x_total[list(clusters.keys())[i]] = np.array(x_over)\n",
    "            y_total[list(clusters.keys())[i]] = np.array(y_over)\n",
    "        else:\n",
    "            x_total[list(clusters.keys())[i]] = np.array([z for z in clusters[list(clusters.keys())[i]]])\n",
    "            y_total[list(clusters.keys())[i]] = np.array([z for z in dict_[list(dict_.keys())[i]]])\n",
    "    return x_total, y_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = [list(np.unique(x)) for x in dict_.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_total, y_total = over_sampling(clusters, dict_, unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## matching process between new id and ref table\n",
    "data_balanced = {}\n",
    "for i, (x,y) in enumerate(zip(x_total, y_total)):\n",
    "    data_balanced[\"{0}\".format(list(x_total.keys())[i])] = y_total[y]\n",
    "data_df_balanced = pd.DataFrame.from_dict(data_balanced, orient='index').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_list_id(df_id, ref_table):\n",
    "    dict_loc = {}; id_total = []\n",
    "    m_total = ref_table.shape[0]\n",
    "    poses = []\n",
    "    for c in df_id:\n",
    "        x = []\n",
    "        for i in range(len(df_id[c].dropna())):\n",
    "            x.append(int(df_id[c][i]))\n",
    "        var = np.array(x)\n",
    "        id_total.append(var)\n",
    "    for i in range(m_total):\n",
    "        key = int(ref_table.iloc[i]['Unique_ID'])\n",
    "        value = ref_table.iloc[i, 0:2].values\n",
    "        dict_loc[key] = value\n",
    "    for i in range(len(id_total)):\n",
    "        pos = []\n",
    "        for j in range(len(id_total[i])):\n",
    "            x = id_total[i][j]\n",
    "            pos.append(dict_loc.get(x))\n",
    "        pos = np.array(pos)\n",
    "        poses.append(pos)    \n",
    "    return id_total, poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_total_balanced, poses_balanced = filter_list_id(data_df_balanced, ref_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(id_total,poses,i):\n",
    "    df_id = pd.DataFrame(id_total[i],columns=[str(i)])\n",
    "    df_pos = pd.DataFrame(poses[i],columns=['x','y'])\n",
    "    return df_pos, df_id\n",
    "\n",
    "df = {}\n",
    "for i, (_id,pose) in enumerate(zip(id_total_balanced,poses_balanced)):\n",
    "    df[list(x_total.keys())[i]] = pd.concat((make_df(id_total_balanced,poses_balanced,i)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained and Tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## classifier for noise detection\n",
    "def classifier(x_train,y_train):       \n",
    "    clf = KNeighborsClassifier(n_neighbors=1)\n",
    "    \n",
    "    metric = ['euclidean']\n",
    "    hyperparameters = {'metric': metric}\n",
    "    \n",
    "    grid = GridSearchCV(estimator = clf,\n",
    "                        param_grid = hyperparameters,\n",
    "                        scoring = 'accuracy',\n",
    "                        cv = 10,\n",
    "                        n_jobs = -1)\n",
    "    \n",
    "    tic = time.time()\n",
    "    grid_result_clf = grid.fit(x_train,y_train)\n",
    "    toc = time.time()\n",
    "    run_time = (toc - tic)/60\n",
    "    return grid_result_clf.best_estimator_, grid_result_clf.best_score_, run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Noise Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_core_nd = np.ones(x_train_cr.values.shape[0])\n",
    "# y_noises_nd = np.zeros(x_train_n.values.shape[0])\n",
    "\n",
    "# x_train_nd = np.append(x_train_cr, x_train_n, axis=0)\n",
    "# y_train_nd = np.append(y_core_nd,y_noises_nd,axis=0)\n",
    "\n",
    "# ## random oversampling\n",
    "# strat = strategy_dict(y_train_nd)\n",
    "# oversample = RandomOverSampler(sampling_strategy=strat)\n",
    "# x_over, y_over = oversample.fit_resample(x_train_nd, y_train_nd)\n",
    "\n",
    "# noises_clf_tuned, noises_clf_score, runtime_noises_clf = classifier(x_over,y_over)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For Cluster Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## train core for cluster prediction\n",
    "# total_c = []\n",
    "# for keys,values in x_total.items():\n",
    "#     for z in values:\n",
    "#         total_c.append(np.append(z.reshape(1,4),keys))\n",
    "# x_pred_clus = np.array(total_c)[:,0:4]\n",
    "# y_pred_clus = np.array(total_c)[:,-1].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clf_tuned, clf_score, runtime_clf = classifier(x_pred_clus, y_pred_clus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regression model for pose prediction\n",
    "def tuning_regr_knn(x_train,y_train):\n",
    "    QUANTITATIVE_COLUMNS = ['x', 'y']\n",
    "    regr = KNeighborsRegressor(n_neighbors=1)\n",
    "    \n",
    "    metric = ['euclidean']\n",
    "    weights = ['uniform', 'distance']\n",
    "    algorithm = ['auto', 'ball_tree', 'kd_tree', 'brute']\n",
    "    param_grid = {'metric': metric}\n",
    "     \n",
    "    grid = GridSearchCV(estimator = regr,\n",
    "                        param_grid = param_grid,\n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        cv = 10,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "    tic = time.time()\n",
    "    grid_result_regr = grid.fit(x_train,y_train[QUANTITATIVE_COLUMNS].values.astype(np.float64))\n",
    "    toc = time.time()\n",
    "    run_time = (toc - tic)/60\n",
    "    return grid_result_regr.best_estimator_, grid_result_regr.best_score_, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_tuned = {}\n",
    "for i,(x,y) in enumerate(zip(x_total,df)):\n",
    "    regr, regr_score, runtime_regr = tuning_regr_knn(x_total[x], df[y])\n",
    "    regr_tuned[\"regr_{0}\".format(list(x_total.keys())[i])] = regr, regr_score, runtime_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Regressor for Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train_n = combined_noise.iloc[:,0:4].values\n",
    "# y_train_n = combined_noise.iloc[:,4:].drop('dbscan',axis=1)\n",
    "\n",
    "# regr_n, regr_score_n, runtime_regr_n = tuning_regr_knn(x_train_n, y_train_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_left['Unique_ID'] = [int(ref_table[(ref_table.x == str(x[0])) & (ref_table.y == str(x[1]))]['Unique_ID'].values) \n",
    "                              for x in y_test_left.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def nn_modified(test_noises, x_train, y_train):\n",
    "    distance_n = []\n",
    "    for x,y in zip(x_train, y_train):\n",
    "        dim = np.array(x).shape[0]\n",
    "        eu_distance = sp.spatial.distance.euclidean(test_noises.reshape(1,dim),np.array(x).reshape(1,dim))\n",
    "        distance_n.append((eu_distance,y))\n",
    "    distance_n = sorted(distance_n)[0][1]\n",
    "    return distance_n\n",
    "\n",
    "## use WKNN instead, return label\n",
    "clus_dbscan = []\n",
    "for n in x_test_left:\n",
    "    clus_dbscan.append(nn_modified(n, x_train, y_train['dbscan'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clus_dbscan = []\n",
    "# for n in x_test_left:\n",
    "#     clus_dbscan.append(int(noises_clf_tuned.predict(n.reshape(1,4))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate index_core and noises\n",
    "def index_nc(labels_):\n",
    "    index_core = []\n",
    "    index_noises = []\n",
    "    for i, x in enumerate(labels_):\n",
    "        if x == -1:\n",
    "            index_noises.append(i) \n",
    "        else:\n",
    "            index_core.append(i)\n",
    "    return index_core, index_noises\n",
    "\n",
    "index_core, index_noises = index_nc(clus_dbscan)\n",
    "\n",
    "test_core = x_test_left[index_core] ## core point testing\n",
    "test_noises = x_test_left[index_noises]\n",
    "\n",
    "label_core = y_test_left.values[index_core] ## core point testing\n",
    "label_noises = y_test_left.values[index_noises]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Core Point Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_kmeans = []\n",
    "for x in test_core:\n",
    "    clus_kmeans.append(final_model.predict(x.reshape(1,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clus_kmeans = []\n",
    "# for x in test_core:\n",
    "#     clus_kmeans.append(clf_tuned.predict(x.reshape(1,4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_core = [int(i) for i in clus_kmeans]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "for i, (x,y) in enumerate(zip(test_core, clus_core)):\n",
    "    data = x.reshape(1, len(x))\n",
    "    result.append(regr_tuned['regr_{0}'.format(int(y))][0].predict(data).astype(int).reshape(2,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(result,columns=['x_pred','y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prep_core = np.append(test_core, np.array(clus_kmeans).reshape(len(clus_kmeans),1).astype(int), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_Regr(): \n",
    "    '''\n",
    "    Generate object for Coordinate Result\n",
    "    '''\n",
    "    def getRegr(self):\n",
    "        return self.regr\n",
    "    def __init__(self, regr):\n",
    "        self.regr = regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(prep_core, regr_tuned):\n",
    "    buff = []\n",
    "    for i,item in enumerate(prep_core):\n",
    "        N = len(item)-1\n",
    "        data = item[0:N].reshape(1,N)\n",
    "        hasil = regr_tuned['regr_{0}'.format(item[-1].astype(int))][0].predict(data)\n",
    "        value = POS_Regr(hasil)\n",
    "        buff.append(value)\n",
    "    return buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred = test_prediction(prep_core, regr_tuned) ## With built in function DBKmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prediction(pred):\n",
    "    df_prediction = []\n",
    "    for i in range(len(pred)):\n",
    "        xs = list(pred[i].getRegr()[0])\n",
    "        df_prediction.append(xs)\n",
    "    dataset = pd.DataFrame(df_prediction,columns=['x_pred','y_pred']) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pred = df_prediction(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Point Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wknn_modified(test_noises, x_train_n, y_train_n, k=5):\n",
    "    distance_n = []; freq = defaultdict(float)\n",
    "    for x,y in zip(x_train_n, y_train_n):\n",
    "        dim = np.array(x).shape[0]\n",
    "        euclidean_distance = sp.spatial.distance.cosine(test_noises.reshape(1,dim),np.array(x).reshape(1,dim))\n",
    "        distance_n.append((euclidean_distance,y))\n",
    "    distance_n = sorted(distance_n)[:k]\n",
    "    count = Counter([b[1] for b in distance_n])\n",
    "    unique_c = list(np.unique(np.array(distance_n)[:,1]).astype(int))\n",
    "    for x in unique_c:\n",
    "        freq.setdefault(int(x),0)\n",
    "    for d in distance_n:\n",
    "        temp = freq[float(d[1])]\n",
    "        if d[0] == float(0):\n",
    "            temp_ = float(temp)\n",
    "            freq[float(d[1])] = temp_\n",
    "        else:\n",
    "            temp_ = float(temp) + (1 / d[0])\n",
    "            freq[float(d[1])] = temp_\n",
    "    for key in freq:\n",
    "        freq[key] = freq[key] / count[key]\n",
    "    return max(freq, key=lambda key: freq[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "### noises point testing\n",
    "x_train_new = x_train_cr.values\n",
    "y_train_new = y_train_cr['Unique_ID'].values \n",
    "\n",
    "## use WKNN instead, return label\n",
    "clus_wknn_n = []\n",
    "for n in test_noises:\n",
    "    clus_wknn_n.append(wknn_modified(n, x_train_new, y_train_new))\n",
    "     \n",
    "pred_n = []\n",
    "for x in clus_wknn_n:\n",
    "    pred_n.append((int(ref_table['x'][ref_table[ref_table.Unique_ID==x].index]),\n",
    "                   int(ref_table['y'][ref_table[ref_table.Unique_ID==x].index])))\n",
    "\n",
    "dataset_n = pd.DataFrame(pred_n,columns=['x_pred','y_pred'])\n",
    "\n",
    "# clus_regr_n = []\n",
    "# for n in test_noises:\n",
    "#     clus_regr_n.append(regr_n.predict(n.reshape(1,4)).reshape(2,)) \n",
    "    \n",
    "# dataset_n = pd.DataFrame(clus_regr_n,columns=['x_pred','y_pred'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def wknn_modified(test_noises, x_train_n, y_train_n, k=25):\n",
    "#     distance_n = []; freq = defaultdict(float)\n",
    "#     for x,y in zip(x_train_n, y_train_n):\n",
    "#         dim = np.array(x).shape[0]\n",
    "#         cosine_distance = sp.spatial.distance.euclidean(test_noises.reshape(1,dim),np.array(x).reshape(1,dim))\n",
    "#         distance_n.append((cosine_distance,y))\n",
    "#     distance_n = sorted(distance_n)[:k]\n",
    "#     distance_n\n",
    "#     unique_c = list(np.unique(np.array(distance_n)[:,1]).astype(int))\n",
    "#     for x in unique_c:\n",
    "#         freq.setdefault(int(x),0)\n",
    "#     for d in distance_n:\n",
    "#         temp = freq[float(d[1])]\n",
    "#         temp_ = temp + (1 / d[0])\n",
    "#         freq[float(d[1])] = temp_\n",
    "#     return max(freq, key=lambda key: freq[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## use WKNN instead, return label\n",
    "# clus_wknn_n = []\n",
    "# for n in test_noises:\n",
    "#     clus_wknn_n.append(wknn_modified(n, x_train_n, y_train_n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_n = []\n",
    "# for x in clus_wknn_n:\n",
    "#     pred_n.append((int(ref_table['x'][ref_table[ref_table.Unique_ID==x].index]),\n",
    "#                    int(ref_table['y'][ref_table[ref_table.Unique_ID==x].index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_n = pd.DataFrame(pred_n,columns=['x_pred','y_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Regression Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regr_error(df_final,label_core):\n",
    "    x, y  = df_final['x_pred'].values, df_final['y_pred'].values\n",
    "    x0, y0 = label_core[:,0].astype(int), label_core[:,1].astype(int)\n",
    "    coords_error = np.sqrt(np.square(x - x0) + np.square(y - y0))\n",
    "    mean_loc_error = coords_error.mean()\n",
    "    return mean_loc_error, coords_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.643231596707641\n"
     ]
    }
   ],
   "source": [
    "mean_loc_error, coords_error = calculate_regr_error(df_pred,label_core)\n",
    "print(mean_loc_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.013925358619815\n"
     ]
    }
   ],
   "source": [
    "# noises point error\n",
    "mean_loc_error_n, coords_error_n = calculate_regr_error(dataset_n,label_noises)\n",
    "print(mean_loc_error_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5144589679174274"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Error gabungan\n",
    "np.sum((len(label_core) * mean_loc_error)+(len(label_noises) * mean_loc_error_n)) / (len(label_core) + len(label_noises))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NicolasEnv] *",
   "language": "python",
   "name": "conda-env-NicolasEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
