{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from numpy import sqrt, square\n",
    "from kneed import KneeLocator\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Training Data\n",
    "path =r\"E:\\PROJECT\\VSCode Python Project\\Sensor Network\\Uji Coba\\Progress Skripsi\\Data\\Data Nico\\Data 15 April Tanpa Gangguan\\Train 2\"\n",
    "globbed_files = glob.glob(path + \"/*.csv\")\n",
    "data = []\n",
    "for csv in globbed_files:\n",
    "    frame = pd.read_csv(csv)\n",
    "    frame['x'] = os.path.basename(csv).split('.')[0][0]\n",
    "    frame['y'] = os.path.basename(csv).split('.')[0][1]\n",
    "    data.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take the lowest number of set data (32)\n",
    "attempt = []\n",
    "for i, item in enumerate(data):\n",
    "    attempt.append(data[i][['Router 1','Router 2','Router 3','Router 4','x','y']]) #.head(32)\n",
    "attempt_concat = pd.concat(attempt)\n",
    "data_train = attempt_concat.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_train[['Router 1','Router 2','Router 3','Router 4']] = data_train[['Router 1','Router 2','Router 3','Router 4']].abs()\n",
    "data_train = data_train.assign(Unique_ID = (data_train['x'].astype(str) + '_' + data_train['y'].astype(str)).astype('category').cat.codes)\n",
    "x_train = data_train.iloc[:,0:4].values\n",
    "y_train = data_train.iloc[:,4:]\n",
    "ref_table = y_train.iloc[:, [0,1,2]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_left =r\"E:\\PROJECT\\VSCode Python Project\\Sensor Network\\Uji Coba\\Progress Skripsi\\Data\\Data Nico\\Data 15 April Tanpa Gangguan\\Test\\Right Test\"\n",
    "globbed_files_left = glob.glob(path_left + \"/*.csv\")\n",
    "data_left = []\n",
    "for csv in globbed_files_left:\n",
    "    frame = pd.read_csv(csv)\n",
    "    frame['x'] = os.path.basename(csv).split('.')[0][0]\n",
    "    frame['y'] = os.path.basename(csv).split('.')[0][1]\n",
    "    data_left.append(frame)\n",
    "    \n",
    "attempt_left = []\n",
    "for i, item in enumerate(data_left):\n",
    "    attempt_left.append(data_left[i][['Router 1','Router 2','Router 3','Router 4','x','y']])\n",
    "attempt_concat_left = pd.concat(attempt_left)\n",
    "data_test_left = attempt_concat_left.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_test_left[['Router 1','Router 2','Router 3','Router 4']] = data_test_left[['Router 1','Router 2','Router 3','Router 4']].abs()\n",
    "data_test_left = data_test_left.tail(562).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split features and label\n",
    "x_test_left = data_test_left.iloc[:,0:4].values\n",
    "y_test_left = data_test_left.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nearest_neighbors = NearestNeighbors(n_neighbors=5)\n",
    "neighbors = nearest_neighbors.fit(x_train)\n",
    "distances, indices = neighbors.kneighbors(x_train)\n",
    "distances = np.sort(distances[:,4], axis=0)\n",
    "i = np.arange(len(distances))\n",
    "knee = KneeLocator(i, distances, S=1, curve='convex', direction='increasing', interp_method='polynomial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=distances[knee.knee], min_samples=5).fit(x_train)\n",
    "labels = db.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) # jumlah cluster, total klas - 1 (kalo ada noise)\n",
    "n_noise_ = list(labels).count(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 24\n",
      "Estimated number of noise points: 57\n"
     ]
    }
   ],
   "source": [
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df = pd.DataFrame(x_train,columns=['PC_1','PC_2','PC_3','PC_4'])\n",
    "ref_table = y_train.iloc[:, [0,1,2]].drop_duplicates()\n",
    "y_train['dbscan'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined = pd.concat((x_train_df,y_train),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_core = combined.loc[~(combined['dbscan'] == -1)]\n",
    "combined_core = combined_core.reset_index(drop=True)\n",
    "\n",
    "combined_noise = combined.loc[combined['dbscan'] == -1]\n",
    "combined_noise = combined_noise.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_cr = combined_core.iloc[:,0:4]\n",
    "y_train_cr = combined_core.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_n = combined_noise.iloc[:,0:4]\n",
    "y_train_n = combined_noise.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handle Imbalaced Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {}\n",
    "clusters = {}\n",
    "for x in np.unique(combined_core.values[:,-1]):\n",
    "    dict_[x] = list(combined_core[combined_core.dbscan == x]['Unique_ID'].values)\n",
    "    clusters[x] = list(combined_core[combined_core.dbscan == x][['PC_1', 'PC_2', 'PC_3', 'PC_4']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_dict(y):\n",
    "    '''\n",
    "    balancing for oversampling strategy\n",
    "    '''\n",
    "    new_strategy = {}\n",
    "    keys = Counter(y).keys()\n",
    "    values = max(Counter(y).values())\n",
    "    for key in keys:\n",
    "        new_strategy[key] = values\n",
    "    return new_strategy\n",
    "\n",
    "## untuk bantu train regressor\n",
    "def over_sampling(clusters, dict_, index):\n",
    "    '''\n",
    "    random oversampling strategy\n",
    "    '''\n",
    "    x_total = {}\n",
    "    y_total = {}\n",
    "    for i,item in enumerate(index):\n",
    "        if len(item) > 1:\n",
    "            x = [z for z in clusters[list(clusters.keys())[i]]]\n",
    "            y = [z for z in dict_[list(dict_.keys())[i]]]\n",
    "            strategy = strategy_dict(y)\n",
    "            oversample = RandomOverSampler(sampling_strategy=strategy)\n",
    "            x_over, y_over = oversample.fit_resample(x, y)\n",
    "            x_total[list(clusters.keys())[i]] = np.array(x_over)\n",
    "            y_total[list(clusters.keys())[i]] = np.array(y_over)\n",
    "        else:\n",
    "            x_total[list(clusters.keys())[i]] = np.array([z for z in clusters[list(clusters.keys())[i]]])\n",
    "            y_total[list(clusters.keys())[i]] = np.array([z for z in dict_[list(dict_.keys())[i]]])\n",
    "    return x_total, y_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = [list(np.unique(x)) for x in dict_.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_total, y_total = over_sampling(clusters, dict_, unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## matching process between new id and ref table\n",
    "data_balanced = {}\n",
    "for i, (x,y) in enumerate(zip(x_total, y_total)):\n",
    "    data_balanced[\"{0}\".format(list(x_total.keys())[i])] = y_total[y]\n",
    "data_df_balanced = pd.DataFrame.from_dict(data_balanced, orient='index').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_list_id(df_id, ref_table):\n",
    "    dict_loc = {}; id_total = []\n",
    "    m_total = ref_table.shape[0]\n",
    "    poses = []\n",
    "    for c in df_id:\n",
    "        x = []\n",
    "        for i in range(len(df_id[c].dropna())):\n",
    "            x.append(int(df_id[c][i]))\n",
    "        var = np.array(x)\n",
    "        id_total.append(var)\n",
    "    for i in range(m_total):\n",
    "        key = int(ref_table.iloc[i]['Unique_ID'])\n",
    "        value = ref_table.iloc[i, 0:2].values\n",
    "        dict_loc[key] = value\n",
    "    for i in range(len(id_total)):\n",
    "        pos = []\n",
    "        for j in range(len(id_total[i])):\n",
    "            x = id_total[i][j]\n",
    "            pos.append(dict_loc.get(x))\n",
    "        pos = np.array(pos)\n",
    "        poses.append(pos)    \n",
    "    return id_total, poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_total_balanced, poses_balanced = filter_list_id(data_df_balanced, ref_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(id_total,poses,i):\n",
    "    df_id = pd.DataFrame(id_total[i],columns=[str(i)])\n",
    "    df_pos = pd.DataFrame(poses[i],columns=['x','y'])\n",
    "    return df_pos, df_id\n",
    "\n",
    "df = {}\n",
    "for i, (_id,pose) in enumerate(zip(id_total_balanced,poses_balanced)):\n",
    "    df[list(x_total.keys())[i]] = pd.concat((make_df(id_total_balanced,poses_balanced,i)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained and Tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regression model for pose prediction\n",
    "def tuning_regr_knn(x_train,y_train):\n",
    "    QUANTITATIVE_COLUMNS = ['x', 'y']\n",
    "    regr = KNeighborsRegressor(n_neighbors=1)\n",
    "    \n",
    "    metric = ['euclidean']\n",
    "    hyperparameters = {'metric': metric}\n",
    "     \n",
    "    grid = GridSearchCV(estimator = regr,\n",
    "                        param_grid = hyperparameters,\n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        cv = 3,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "    tic = time.time()\n",
    "    grid_result_regr = grid.fit(x_train,y_train[QUANTITATIVE_COLUMNS].values.astype(np.float64))\n",
    "    toc = time.time()\n",
    "    run_time = (toc - tic)/60\n",
    "    return grid_result_regr.best_estimator_, grid_result_regr.best_score_, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_tuned = {}\n",
    "for i,(x,y) in enumerate(zip(x_total,df)):\n",
    "    regr, regr_score, runtime_regr = tuning_regr_knn(x_total[x], df[y])\n",
    "    regr_tuned[\"regr_{0}\".format(list(x_total.keys())[i])] = regr, regr_score, runtime_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_left['Unique_ID'] = [int(ref_table[(ref_table.x == str(x[0])) & (ref_table.y == str(x[1]))]['Unique_ID'].values) \n",
    "                              for x in y_test_left.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def nn_modified(test_noises, x_train, y_train):\n",
    "    distance_n = []\n",
    "    for x,y in zip(x_train, y_train):\n",
    "        dim = np.array(x).shape[0]\n",
    "        eu_distance = sp.spatial.distance.euclidean(test_noises.reshape(1,dim),np.array(x).reshape(1,dim))\n",
    "        distance_n.append((eu_distance,y))\n",
    "    clus = sorted(distance_n)[0][1]\n",
    "    return clus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "clus_knn = []\n",
    "for n in x_test_left:\n",
    "    clus_knn.append(nn_modified(n, x_train, y_train['dbscan'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate index_core and noises from function with cosine similiarity\n",
    "def index_nc(labels_):\n",
    "    index_core = []\n",
    "    index_noises = []\n",
    "    for i, x in enumerate(labels_):\n",
    "        if x == -1:\n",
    "            index_noises.append(i) \n",
    "        else:\n",
    "            index_core.append(i)\n",
    "    return index_core, index_noises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_core, index_noises = index_nc(clus_knn)\n",
    "\n",
    "test_core = x_test_left[index_core] ## core point testing\n",
    "test_noises = x_test_left[index_noises]\n",
    "\n",
    "label_core = y_test_left.values[index_core] ## core point testing\n",
    "label_noises = y_test_left.values[index_noises]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_core = np.append(test_core, np.array(clus_knn)[index_core].reshape(len(np.array(clus_knn)[index_core]),1).astype(int), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_Regr(): \n",
    "    '''\n",
    "    Generate object for Coordinate Result\n",
    "    '''\n",
    "    def getRegr(self):\n",
    "        return self.regr\n",
    "    def __init__(self, regr):\n",
    "        self.regr = regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(prep_core, regr_tuned):\n",
    "    buff = []\n",
    "    for i,item in enumerate(prep_core):\n",
    "        N = len(item)-1\n",
    "        data = item[0:N].reshape(1,N)\n",
    "        hasil = regr_tuned['regr_{0}'.format(item[-1].astype(int))][0].predict(data)\n",
    "        value = POS_Regr(hasil)\n",
    "        buff.append(value)\n",
    "    return buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test_prediction(prep_core, regr_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prediction(pred):\n",
    "    df_prediction = []\n",
    "    for i in range(len(pred)):\n",
    "        xs = list(pred[i].getRegr()[0])\n",
    "        df_prediction.append(xs)\n",
    "    dataset = pd.DataFrame(df_prediction,columns=['x_pred','y_pred']) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_prediction(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noise Testing Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_n = x_train_cr.values\n",
    "y_train_n = y_train_cr['Unique_ID'].values ## label kelas grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wknn_modified(test_noises, x_train_n, y_train_n, k=5):\n",
    "    distance_n = []; freq = defaultdict(float)\n",
    "    for x,y in zip(x_train_n, y_train_n):\n",
    "        dim = np.array(x).shape[0]\n",
    "        eu_distance = sp.spatial.distance.euclidean(test_noises.reshape(1,dim),np.array(x).reshape(1,dim))\n",
    "        distance_n.append((eu_distance,y))\n",
    "    distance_n = sorted(distance_n)[:k]\n",
    "    count = Counter([b[1] for b in distance_n])\n",
    "    unique_c = list(np.unique(np.array(distance_n)[:,1]).astype(int))\n",
    "    for x in unique_c:\n",
    "        freq.setdefault(int(x),0)\n",
    "    for d in distance_n:\n",
    "        temp = freq[float(d[1])]\n",
    "        if d[0] == float(0):\n",
    "            temp_ = float(temp)\n",
    "            freq[float(d[1])] = temp_\n",
    "        else:\n",
    "            temp_ = float(temp) + (1 / d[0])\n",
    "            freq[float(d[1])] = temp_\n",
    "    for key in freq:\n",
    "        freq[key] = freq[key] / count[key]\n",
    "    return max(freq, key=lambda key: freq[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use WKNN instead, return label\n",
    "clus_wknn_n = []\n",
    "for n in test_noises:\n",
    "    clus_wknn_n.append(wknn_modified(n, x_train, y_train['Unique_ID'].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_n = []\n",
    "for x in clus_wknn_n:\n",
    "    pred_n.append((int(ref_table['x'][ref_table[ref_table.Unique_ID==x].index]),\n",
    "                   int(ref_table['y'][ref_table[ref_table.Unique_ID==x].index])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_n = pd.DataFrame(pred_n,columns=['x_pred','y_pred'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Regression Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regr_error(df_final,label_core):\n",
    "    x, y  = df_final['x_pred'].values, df_final['y_pred'].values\n",
    "    x0, y0 = label_core[:,0].astype(int), label_core[:,1].astype(int)\n",
    "    coords_error = np.sqrt(np.square(x - x0) + np.square(y - y0))\n",
    "    mean_loc_error = coords_error.mean()\n",
    "    return mean_loc_error, coords_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4701089027594252\n"
     ]
    }
   ],
   "source": [
    "## core point error\n",
    "mean_loc_error, coords_error = calculate_regr_error(df_pred,label_core)\n",
    "print(mean_loc_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.832242049571854\n"
     ]
    }
   ],
   "source": [
    "## noises point error\n",
    "mean_loc_error_n, coords_error_n = calculate_regr_error(dataset_n,label_noises)\n",
    "print(mean_loc_error_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.5448552462296061"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Error gabungan\n",
    "np.sum((len(label_core) * mean_loc_error)+(len(label_noises) * mean_loc_error_n)) / (len(label_core) + len(label_noises))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.41421356, 2.23606798, 3.        , 2.82842712, 2.        ,\n",
       "       3.16227766, 0.        , 1.41421356, 3.16227766, 0.        ,\n",
       "       2.        , 0.        , 1.41421356, 1.41421356, 0.        ,\n",
       "       2.23606798, 2.        , 2.82842712, 2.23606798, 3.        ,\n",
       "       0.        , 1.41421356, 3.        , 0.        , 1.41421356,\n",
       "       2.        , 2.        , 3.16227766, 1.41421356, 1.41421356,\n",
       "       1.41421356, 0.        , 3.        , 3.        , 3.        ,\n",
       "       1.41421356, 0.        , 3.16227766, 1.41421356, 2.        ,\n",
       "       0.        , 3.        , 1.41421356, 2.        , 3.16227766,\n",
       "       1.41421356, 2.        , 2.        , 0.        , 1.41421356,\n",
       "       2.        , 2.        , 0.        , 2.23606798, 2.        ,\n",
       "       0.        , 3.        , 1.41421356, 1.41421356, 1.41421356,\n",
       "       3.        , 3.16227766, 0.        , 2.23606798, 1.41421356,\n",
       "       2.23606798, 0.        , 1.41421356, 0.        , 2.        ,\n",
       "       2.23606798, 0.        , 3.        , 0.        , 2.        ,\n",
       "       0.        , 0.        , 3.16227766, 1.41421356, 2.        ,\n",
       "       0.        , 2.        , 0.        , 1.41421356, 3.16227766,\n",
       "       3.16227766, 1.41421356, 0.        , 1.41421356, 1.41421356,\n",
       "       2.        , 0.        , 3.16227766, 3.        , 2.        ,\n",
       "       2.        , 3.16227766, 0.        , 1.41421356, 2.23606798,\n",
       "       1.41421356, 0.        , 1.41421356, 2.        , 1.41421356,\n",
       "       1.41421356, 0.        , 0.        , 2.        , 0.        ,\n",
       "       2.        , 2.        , 1.41421356, 0.        , 3.        ,\n",
       "       1.41421356, 1.41421356, 0.        , 1.41421356, 2.        ,\n",
       "       2.        , 0.        , 0.        , 3.        , 1.41421356,\n",
       "       0.        , 1.41421356, 1.41421356, 3.        , 1.41421356,\n",
       "       0.        , 2.        , 3.        , 1.41421356, 0.        ,\n",
       "       0.        , 2.        , 2.82842712, 2.23606798, 1.41421356,\n",
       "       3.16227766, 0.        , 3.16227766, 0.        , 0.        ,\n",
       "       1.41421356, 3.        , 1.41421356, 1.41421356, 0.        ,\n",
       "       2.        , 3.16227766, 3.        , 2.82842712, 1.41421356,\n",
       "       1.41421356, 0.        , 1.41421356, 0.        , 2.        ,\n",
       "       1.41421356, 1.41421356, 0.        , 3.        , 1.41421356,\n",
       "       2.        , 3.        , 1.41421356, 3.        , 2.23606798,\n",
       "       3.16227766, 2.        , 3.16227766, 2.23606798, 0.        ,\n",
       "       0.        , 1.41421356, 3.16227766, 2.        , 1.41421356,\n",
       "       1.41421356, 0.        , 0.        , 1.41421356, 1.41421356,\n",
       "       1.41421356, 3.16227766, 0.        , 3.        , 3.        ,\n",
       "       3.16227766, 0.        , 1.41421356, 1.41421356, 3.16227766,\n",
       "       1.41421356, 0.        , 2.        , 3.16227766, 3.        ,\n",
       "       2.        , 0.        , 2.        , 3.16227766, 2.23606798,\n",
       "       2.        , 1.41421356, 0.        , 1.41421356, 0.        ,\n",
       "       1.41421356, 0.        , 0.        , 1.41421356, 2.23606798,\n",
       "       1.41421356, 0.        , 2.23606798, 1.41421356, 1.41421356,\n",
       "       0.        , 3.        , 0.        , 0.        , 3.        ,\n",
       "       1.41421356, 1.41421356, 0.        , 0.        , 1.41421356,\n",
       "       1.41421356, 1.41421356, 0.        , 3.16227766, 1.41421356,\n",
       "       3.16227766, 2.23606798, 2.        , 1.41421356, 0.        ,\n",
       "       2.        , 1.41421356, 0.        , 2.        , 1.41421356,\n",
       "       2.        , 0.        , 0.        , 3.16227766, 3.        ,\n",
       "       2.82842712, 1.41421356, 0.        , 0.        , 2.23606798,\n",
       "       0.        , 1.41421356, 1.41421356, 1.41421356, 0.        ,\n",
       "       2.        , 2.23606798, 2.        , 1.41421356, 2.        ,\n",
       "       1.41421356, 0.        , 1.41421356, 2.        , 3.        ,\n",
       "       1.41421356, 1.41421356, 3.        , 0.        , 1.41421356,\n",
       "       2.        , 3.        , 2.        , 1.41421356, 0.        ,\n",
       "       2.        , 0.        , 1.41421356, 3.        , 0.        ,\n",
       "       0.        , 0.        , 1.41421356, 1.41421356, 1.41421356,\n",
       "       0.        , 1.41421356, 3.        , 0.        , 0.        ,\n",
       "       3.        , 1.41421356, 2.23606798, 0.        , 2.        ,\n",
       "       1.        , 1.41421356, 2.23606798, 3.        , 0.        ,\n",
       "       2.        , 2.        , 2.        , 2.        , 0.        ,\n",
       "       1.41421356, 1.41421356, 2.        , 1.41421356, 0.        ,\n",
       "       1.41421356, 2.        , 1.41421356, 0.        , 0.        ,\n",
       "       2.        , 0.        , 3.        , 2.23606798, 0.        ,\n",
       "       0.        , 2.23606798, 0.        , 2.        , 1.41421356,\n",
       "       2.23606798, 3.16227766, 3.16227766, 3.16227766, 2.23606798,\n",
       "       2.23606798, 0.        , 0.        , 0.        , 2.        ,\n",
       "       0.        , 2.        , 0.        , 1.41421356, 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.41421356, 0.        ,\n",
       "       1.41421356, 1.41421356, 1.41421356, 2.        , 2.82842712,\n",
       "       3.16227766, 2.        , 1.41421356, 0.        , 3.        ,\n",
       "       2.        , 3.        , 0.        , 1.41421356, 1.41421356,\n",
       "       2.        , 0.        , 3.        , 0.        , 3.16227766,\n",
       "       3.        , 1.41421356, 2.23606798, 3.16227766, 0.        ,\n",
       "       2.        , 0.        , 2.        , 1.41421356, 3.        ,\n",
       "       1.41421356, 0.        , 0.        , 2.23606798, 0.        ,\n",
       "       0.        , 2.        , 1.41421356, 0.        , 3.16227766,\n",
       "       0.        , 0.        , 1.41421356, 2.82842712, 0.        ,\n",
       "       1.41421356, 2.        , 0.        , 0.        , 1.41421356,\n",
       "       2.23606798, 1.41421356, 0.        , 2.        , 1.41421356,\n",
       "       3.16227766, 1.41421356, 3.16227766, 0.        , 3.        ,\n",
       "       0.        , 1.41421356, 1.41421356, 1.41421356, 2.        ,\n",
       "       3.16227766, 1.41421356, 1.41421356, 0.        , 1.41421356,\n",
       "       0.        , 2.        , 1.41421356, 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.41421356, 2.        , 3.16227766,\n",
       "       0.        , 2.23606798, 1.41421356, 3.        , 0.        ,\n",
       "       1.41421356, 2.23606798, 1.41421356, 2.        , 1.41421356,\n",
       "       0.        , 2.        , 3.16227766, 0.        , 3.        ,\n",
       "       2.        , 1.41421356, 2.23606798, 1.        , 2.23606798,\n",
       "       2.        , 2.        , 2.23606798, 2.        , 2.        ,\n",
       "       2.        , 1.        , 2.        , 1.41421356, 2.23606798,\n",
       "       2.        , 2.82842712, 2.23606798, 2.23606798, 2.        ,\n",
       "       2.        , 1.        , 1.41421356, 2.        , 1.41421356,\n",
       "       2.23606798, 2.23606798, 2.23606798, 2.        , 2.23606798,\n",
       "       2.23606798, 2.        , 1.41421356, 2.        , 2.        ,\n",
       "       2.        , 1.41421356, 1.        , 0.        , 2.23606798,\n",
       "       2.        , 2.        , 1.41421356, 2.        , 2.        ,\n",
       "       2.23606798, 2.        , 2.        , 2.23606798, 1.        ,\n",
       "       2.23606798, 2.        , 2.        , 1.41421356, 2.23606798,\n",
       "       2.23606798, 1.        , 1.        , 2.23606798, 2.        ,\n",
       "       2.        , 1.41421356, 2.        , 1.41421356, 1.        ,\n",
       "       2.        , 2.        , 1.        , 2.23606798, 2.23606798,\n",
       "       2.23606798, 2.        , 1.        , 2.23606798, 2.        ,\n",
       "       2.        , 1.41421356, 2.23606798, 2.        , 2.        ,\n",
       "       1.        , 2.23606798, 2.        , 2.23606798, 2.23606798,\n",
       "       2.23606798, 2.        , 1.        , 2.23606798, 1.41421356,\n",
       "       2.        , 2.23606798, 2.        , 2.23606798, 2.        ,\n",
       "       1.41421356, 1.41421356, 2.23606798, 2.        , 2.        ,\n",
       "       2.23606798, 1.        , 2.23606798, 1.        , 2.        ,\n",
       "       2.        , 1.41421356, 2.23606798, 2.        , 2.        ,\n",
       "       2.23606798, 2.        , 1.        , 0.        , 2.23606798,\n",
       "       1.        , 2.        ])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.append(coords_error, coords_error_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NicolasEnv] *",
   "language": "python",
   "name": "conda-env-NicolasEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
