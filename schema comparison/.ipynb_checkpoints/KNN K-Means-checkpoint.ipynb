{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from random import randint\n",
    "from numpy import sqrt, square\n",
    "from kneed import KneeLocator\n",
    "from collections import Counter\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsRegressor, KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Training Data\n",
    "path =r\"E:\\PROJECT\\VSCode Python Project\\Sensor Network\\Uji Coba\\Progress Skripsi\\Data\\Data Nico\\Data 20 April Fluctuative Noise Quadrant 3 (3,3)\\Train\"\n",
    "globbed_files = glob.glob(path + \"/*.csv\")\n",
    "data = []\n",
    "for csv in globbed_files:\n",
    "    frame = pd.read_csv(csv)\n",
    "    frame['x'] = os.path.basename(csv).split('.')[0][0]\n",
    "    frame['y'] = os.path.basename(csv).split('.')[0][1]\n",
    "    data.append(frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Take the lowest number of set data (32)\n",
    "attempt = []\n",
    "for i, item in enumerate(data):\n",
    "    attempt.append(data[i][['Router 1','Router 2','Router 3','Router 4','x','y']]) #.head(32)\n",
    "attempt_concat = pd.concat(attempt)\n",
    "data_train = attempt_concat.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_train[['Router 1','Router 2','Router 3','Router 4']] = data_train[['Router 1','Router 2','Router 3','Router 4']].abs()\n",
    "data_train = data_train.assign(Unique_ID = (data_train['x'].astype(str) + '_' + data_train['y'].astype(str)).astype('category').cat.codes)\n",
    "x_train = data_train.iloc[:,0:4].values\n",
    "y_train = data_train.iloc[:,4:]\n",
    "ref_table = y_train.iloc[:, [0,1,2]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Left Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_left =r\"E:\\PROJECT\\VSCode Python Project\\Sensor Network\\Uji Coba\\Progress Skripsi\\Data\\Data Nico\\Data 20 April Fluctuative Noise Quadrant 3 (3,3)\\Test\\Right Test\"\n",
    "globbed_files_left = glob.glob(path_left + \"/*.csv\")\n",
    "data_left = []\n",
    "for csv in globbed_files_left:\n",
    "    frame = pd.read_csv(csv)\n",
    "    frame['x'] = os.path.basename(csv).split('.')[0][0]\n",
    "    frame['y'] = os.path.basename(csv).split('.')[0][1]\n",
    "    data_left.append(frame)\n",
    "    \n",
    "attempt_left = []\n",
    "for i, item in enumerate(data_left):\n",
    "    attempt_left.append(data_left[i][['Router 1','Router 2','Router 3','Router 4','x','y']])\n",
    "attempt_concat_left = pd.concat(attempt_left)\n",
    "data_test_left = attempt_concat_left.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "data_test_left[['Router 1','Router 2','Router 3','Router 4']] = data_test_left[['Router 1','Router 2','Router 3','Router 4']].abs()\n",
    "data_test_left = data_test_left.tail(562).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## split features and label\n",
    "x_test_left = data_test_left.iloc[:,0:4].values\n",
    "y_test_left = data_test_left.iloc[:,4:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "ks = range(1,25) ## batas maksimal kelas\n",
    "inertias_values = []  ## or so called SSE\n",
    "for k in ks:\n",
    "    model = KMeans(n_clusters=k)     # Create a KMeans instance with k clusters: model\n",
    "    model.fit(x_train)      # Fit model to samples\n",
    "    inertias_values.append(model.inertia_)        # Append the inertia to the list of inertias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kneed import KneeLocator\n",
    "kl = KneeLocator(range(1,25), inertias_values, curve=\"convex\", direction=\"decreasing\")\n",
    "kl.elbow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Param Gained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Tuple\n",
    "\n",
    "# def _initialize_clusters(k: int) -> dict:\n",
    "#     return {str(i): [] for i in range(1, k + 1)}\n",
    "\n",
    "# def l2norm(point, center) -> float:\n",
    "#     \"\"\"calculate euclidean distance between two n-dimensional points\"\"\"\n",
    "#     s = 0\n",
    "#     for x, y in zip(point, center):\n",
    "#         s += (x - y) ** 2\n",
    "#     d = s[0] ** (1 / 2)\n",
    "#     return d\n",
    "\n",
    "# def l2norm_vector(point, centers) -> np.ndarray:\n",
    "#     \"\"\"calculate l2-norm between a point and several cluster centers.\"\"\"\n",
    "#     l2 = []\n",
    "#     for center in centers:\n",
    "#         center = center.reshape(1, len(center)).T  # (n, 1)\n",
    "#         norm = l2norm(point, center)\n",
    "#         l2.append(norm)\n",
    "#     return np.array(l2)\n",
    "\n",
    "# class Kmeans:\n",
    "#     def __init__(self,\n",
    "#                  n_clusters: int,          # number of clusters assignments\n",
    "#                  data: np.ndarray,         # data matrix (n x d)\n",
    "#                  iterations: int = 100    # number of iterations to run.\n",
    "#                  ): \n",
    "#         self.n_clusters = n_clusters\n",
    "#         self.data = data\n",
    "#         self.iterations = iterations\n",
    "#         self.clusters = _initialize_clusters(self.n_clusters)  # define cluster dictionary for training data\n",
    "#         index = np.random.choice(self.data.shape[0], self.n_clusters, replace=False)\n",
    "#         self.centroids = self.data[:,:4][index]\n",
    "        \n",
    "#     def train(self, method: str = \"l2\", verbose: bool = True) -> Tuple[np.ndarray, dict]:\n",
    "#         iterations = self.iterations  # allows for multiple training runs if user desires\n",
    "        \n",
    "#         if method == \"l2\":\n",
    "#             self._train_l2(iterations)\n",
    "#         elif method == \"l1\":\n",
    "#             self._train_l1(iterations, verbose=verbose)\n",
    "            \n",
    "#         if verbose:\n",
    "#             print(\"Training done...\")\n",
    "#         return self.centroids, self.clusters\n",
    "    \n",
    "#     def predict(self, x: np.ndarray, method: str = \"l2\") -> Tuple[dict, np.ndarray]:\n",
    "#         \"\"\"calculate closest distance between input matrix and cluster assignments\"\"\"\n",
    "#         # initialize cluster dictionary.\n",
    "#         #clusters = _initialize_clusters(self.n_clusters)\n",
    "#         if method == \"l2\":\n",
    "#             for i in range(len(x)):\n",
    "#                 d = l2norm_vector(x[i], self.centroids)\n",
    "#                 c = np.argmin(d) + 1\n",
    "#                 #clusters[str(c)].append(x[i])\n",
    "#         elif method == \"l1\":\n",
    "#             for i in range(len(x)):\n",
    "#                 d = l1norm_vector(x[i], self.centroids)\n",
    "#                 c = np.argmin(d) + 1\n",
    "#                 #clusters[str(c)].append(x[i])            \n",
    "#         return c #clusters, self.centroids\n",
    "                \n",
    "#     def _train_l2(self, iterations: int):\n",
    "#         \"\"\"run training using l2-norm\"\"\"\n",
    "#         i = 0\n",
    "#         while iterations != 0:\n",
    "#             for j in range(self.n_clusters):\n",
    "#                 self.clusters[str(j + 1)] = []\n",
    "#             for i in range(len(self.data)):\n",
    "#                 d = l2norm_vector(self.data[:,:4][i], self.centroids)\n",
    "#                 c = np.argmin(d) + 1\n",
    "#                 self.clusters[str(c)].append((self.data[:,:4][i], self.data[:,4][i]))\n",
    "\n",
    "#             old_centroids = self.centroids.copy()\n",
    "#             for i, c in enumerate(self.clusters.keys()):\n",
    "#                 mean = np.mean(np.array([x[0] for x in self.clusters[str(c)]]), axis=0)\n",
    "#                 self.centroids[int(i)] = mean\n",
    "\n",
    "#             print(\"\\riterations: {}...\".format(iterations))\n",
    "#             iterations -= 1\n",
    "#             i += 1\n",
    "#             if np.all(old_centroids == self.centroids):\n",
    "#                 break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = data_train.drop(['x','y'], axis=1).values\n",
    "# K_l2 = Kmeans(n_clusters = kl.elbow, data = data)\n",
    "# centroids_l2, clusters_l2 = K_l2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def core_combine(clusters, ref_table):\n",
    "#     value_ = []\n",
    "#     unique_ = []\n",
    "#     keys_ = []\n",
    "#     for key,values in clusters.items():\n",
    "#         for value in values:\n",
    "#             value_.append(value[0]) \n",
    "#             unique_.append(value[1])\n",
    "#             keys_.append(key)\n",
    "    \n",
    "#     df_unique_keys = pd.DataFrame({'Unique_ID': unique_,'kmeans': keys_}).astype(int)\n",
    "\n",
    "#     coord_ = []\n",
    "#     for x in df_unique_keys['Unique_ID'].values:\n",
    "#         coord_.append(ref_table[ref_table.Unique_ID == x][['x','y']].values.reshape(2,))\n",
    "\n",
    "#     df_coord = pd.DataFrame(np.array(coord_),columns=['x','y'])\n",
    "#     df_rssi_value = pd.DataFrame(value_,columns=['PC_1','PC_2','PC_3','PC_4'])\n",
    "#     combined_cr = pd.concat([df_rssi_value, df_coord, df_unique_keys],axis=1)\n",
    "    \n",
    "#     return combined_cr\n",
    "\n",
    "# combined_cr = core_combine(clusters_l2, ref_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data_train.drop(['x','y','Unique_ID'], axis=1).values\n",
    "final_model = KMeans(n_clusters=kl.elbow,random_state=1)\n",
    "final_model.fit(data)\n",
    "\n",
    "data_train['kmeans'] = final_model.labels_\n",
    "centroid = final_model.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_cr = data_train.rename(columns={'Router 1': 'PC_1', 'Router 2': 'PC_2', 'Router 3': 'PC_3', 'Router 4': 'PC_4'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imbalanced Data Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def strategy_dict(y):\n",
    "    '''\n",
    "    balancing for oversampling strategy\n",
    "    '''\n",
    "    new_strategy = {}\n",
    "    keys = Counter(y).keys()\n",
    "    values = max(Counter(y).values())\n",
    "    for key in keys:\n",
    "        new_strategy[key] = values\n",
    "    return new_strategy\n",
    "\n",
    "## untuk bantu train regressor\n",
    "def over_sampling(clusters, dict_, index):\n",
    "    '''\n",
    "    random oversampling strategy\n",
    "    '''\n",
    "    x_total = {}\n",
    "    y_total = {}\n",
    "    for i,item in enumerate(index):\n",
    "        if len(item) > 1:\n",
    "            x = [z for z in clusters[list(clusters.keys())[i]]]\n",
    "            y = [z for z in dict_[list(dict_.keys())[i]]]\n",
    "            strategy = strategy_dict(y)\n",
    "            oversample = RandomOverSampler(sampling_strategy=strategy)\n",
    "            x_over, y_over = oversample.fit_resample(x, y)\n",
    "            x_total[list(clusters.keys())[i]] = np.array(x_over)\n",
    "            y_total[list(clusters.keys())[i]] = np.array(y_over)\n",
    "        else:\n",
    "            x_total[list(clusters.keys())[i]] = np.array([z for z in clusters[list(clusters.keys())[i]]])\n",
    "            y_total[list(clusters.keys())[i]] = np.array([z for z in dict_[list(dict_.keys())[i]]])\n",
    "    return x_total, y_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_ = {}\n",
    "clusters = {}\n",
    "for x in np.unique(combined_cr.values[:,-1]):\n",
    "    dict_[x] = list(combined_cr[combined_cr.kmeans == x]['Unique_ID'].values)\n",
    "    clusters[x] = list(combined_cr[combined_cr.kmeans == x][['PC_1','PC_2','PC_3','PC_4']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique = [list(np.unique(x)) for x in dict_.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_total, y_total = over_sampling(clusters, dict_, unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## matching process between new id and ref table\n",
    "data_balanced = {}\n",
    "for i, (x,y) in enumerate(zip(x_total, y_total)):\n",
    "    data_balanced[\"{0}\".format(list(x_total.keys())[i])] = y_total[y]\n",
    "data_df_balanced = pd.DataFrame.from_dict(data_balanced, orient='index').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_list_id(df_id, ref_table):\n",
    "    dict_loc = {}; id_total = []\n",
    "    m_total = ref_table.shape[0]\n",
    "    poses = []\n",
    "    for c in df_id:\n",
    "        x = []\n",
    "        for i in range(len(df_id[c].dropna())):\n",
    "            x.append(int(df_id[c][i]))\n",
    "        var = np.array(x)\n",
    "        id_total.append(var)\n",
    "    for i in range(m_total):\n",
    "        key = int(ref_table.iloc[i]['Unique_ID'])\n",
    "        value = ref_table.iloc[i, 0:2].values\n",
    "        dict_loc[key] = value\n",
    "    for i in range(len(id_total)):\n",
    "        pos = []\n",
    "        for j in range(len(id_total[i])):\n",
    "            x = id_total[i][j]\n",
    "            pos.append(dict_loc.get(x))\n",
    "        pos = np.array(pos)\n",
    "        poses.append(pos)    \n",
    "    return id_total, poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_total_balanced, poses_balanced = filter_list_id(data_df_balanced, ref_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_df(id_total,poses,i):\n",
    "    df_id = pd.DataFrame(id_total[i],columns=[str(i+1)])\n",
    "    df_pos = pd.DataFrame(poses[i],columns=['x','y'])\n",
    "    return df_pos, df_id\n",
    "\n",
    "df = {}\n",
    "for i, (_id,pose) in enumerate(zip(id_total_balanced,poses_balanced)):\n",
    "    df[list(x_total.keys())[i]] = pd.concat((make_df(id_total_balanced,poses_balanced,i)),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Trained and Tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### -- Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## regression model for pose prediction\n",
    "def tuning_regr_knn(x_train,y_train):\n",
    "    QUANTITATIVE_COLUMNS = ['x', 'y']\n",
    "    regr = KNeighborsRegressor(n_neighbors=1)\n",
    "    \n",
    "    metric = ['euclidean']\n",
    "    hyperparameters = {'metric': metric}\n",
    "     \n",
    "    grid = GridSearchCV(estimator = regr,\n",
    "                        param_grid = hyperparameters,\n",
    "                        scoring = 'neg_mean_squared_error',\n",
    "                        cv = 10,\n",
    "                        n_jobs = -1)\n",
    "\n",
    "    tic = time.time()\n",
    "    grid_result_regr = grid.fit(x_train,y_train[QUANTITATIVE_COLUMNS].values.astype(np.float64))\n",
    "    toc = time.time()\n",
    "    run_time = (toc - tic)/60\n",
    "    return grid_result_regr.best_estimator_, grid_result_regr.best_score_, run_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr_tuned = {}\n",
    "for i,(x,y) in enumerate(zip(x_total,df)):\n",
    "    regr, regr_score, runtime_regr = tuning_regr_knn(x_total[x], df[y])\n",
    "    regr_tuned[\"regr_{0}\".format(list(x_total.keys())[i])] = regr, regr_score, runtime_regr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_left['Unique_ID'] = [int(ref_table[(ref_table.x == str(x[0])) & (ref_table.y == str(x[1]))]['Unique_ID'].values) \n",
    "                              for x in y_test_left.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preparation_core(test_core, model):\n",
    "#     clusters_final = []\n",
    "#     for i,x in enumerate(test_core):\n",
    "#         test_point = np.array(x).reshape(1,4)\n",
    "#         result = model.predict(test_point)\n",
    "#         clusters_final.append(result)\n",
    "#     prep_core = np.append(test_core, np.array(clusters_final).reshape(len(clusters_final),1).astype(int), axis=1)\n",
    "#     return prep_core\n",
    "\n",
    "# prep_core = preparation_core(x_test_left, K_l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparation_core(test_core, model):\n",
    "    clusters_final = []\n",
    "    for i,x in enumerate(test_core):\n",
    "        test_point = np.array(x).reshape(1,4)\n",
    "        result = model.predict(test_point)\n",
    "        clusters_final.append(result)\n",
    "    prep_core = np.append(test_core, np.array(clusters_final).astype(int), axis=1)\n",
    "    return prep_core\n",
    "\n",
    "prep_core = preparation_core(x_test_left, final_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class POS_Regr(): \n",
    "    '''\n",
    "    Generate object for Coordinate Result\n",
    "    '''\n",
    "    def getRegr(self):\n",
    "        return self.regr\n",
    "    def __init__(self, regr):\n",
    "        self.regr = regr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(prep_core, regr_tuned):\n",
    "    buff = []\n",
    "    for i,item in enumerate(prep_core):\n",
    "        N = len(item)-1\n",
    "        data = item[0:N].reshape(1,N)\n",
    "        hasil = regr_tuned['regr_{0}'.format(item[-1].astype(int))][0].predict(data)\n",
    "        value = POS_Regr(hasil)\n",
    "        buff.append(value)\n",
    "    return buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = test_prediction(prep_core, regr_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_prediction(pred):\n",
    "    df_prediction = []\n",
    "    for i in range(len(pred)):\n",
    "        xs = list(pred[i].getRegr()[0])\n",
    "        df_prediction.append(xs)\n",
    "    dataset = pd.DataFrame(df_prediction,columns=['x_pred','y_pred']) \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = df_prediction(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Regression Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_regr_error(df_final,label_core):\n",
    "    x, y  = df_final['x_pred'].values, df_final['y_pred'].values\n",
    "    x0, y0 = label_core[:,0].astype(int), label_core[:,1].astype(int)\n",
    "    coords_error = np.sqrt(np.square(x - x0) + np.square(y - y0))\n",
    "    mean_loc_error = coords_error.mean()\n",
    "    return mean_loc_error, coords_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09964243396722557\n"
     ]
    }
   ],
   "source": [
    "mean_loc_error, coords_error = calculate_regr_error(df_pred,y_test_left.values)\n",
    "print(mean_loc_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.41421356, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.41421356, 2.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.41421356, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.41421356, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.41421356, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.41421356, 0.        , 0.        , 2.23606798,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.41421356, 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.        , 0.        , 0.        , 1.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.41421356, 0.        , 1.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 2.        , 2.        , 0.        , 0.        ,\n",
       "       1.41421356, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.41421356, 0.        , 0.        , 0.        , 1.41421356,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.41421356, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 1.41421356, 0.        ,\n",
       "       2.23606798, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 2.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.41421356, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.41421356, 0.        , 1.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.41421356, 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       1.41421356, 0.        , 2.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.41421356, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 1.41421356, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.41421356,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 2.82842712, 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 1.41421356,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        ])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coords_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:NicolasEnv] *",
   "language": "python",
   "name": "conda-env-NicolasEnv-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
